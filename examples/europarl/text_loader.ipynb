{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from frostings.loader import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_samples(samples):\n",
    "    # remove input sentences that are too short or too long\n",
    "    samples = [((x, l1), (t, l2)) for (x, l1), (t, l2) in samples if len(x) > 1 and len(x) <= 400]\n",
    "\n",
    "    # remove target sentences that are too short or too long\n",
    "    samples = [((x, l1), (t, l2)) for (x, l1), (t, l2) in samples if len(t) > 1 and len(t) <= 450]\n",
    "\n",
    "    return samples\n",
    "\n",
    "# prepare a dictionary for mapping characters to integer tokens\n",
    "\n",
    "def get_dictionary_char(lang = 'en'):\n",
    "    with open('./data/alphabet.' + lang, 'r') as f:\n",
    "        alphabet_raw = f.read().replace('\\r\\n', '\\n').replace('\\r', '\\n') # removing microsoft formatting\n",
    "        alphabet = list(set(alphabet_raw)) # removing duplicate entries\n",
    "    return {character: idx for idx, character in enumerate(alphabet)}\n",
    "\n",
    "def char_encoding(sentence, lang = 'en'):\n",
    "    alphadict = get_dictionary_char(lang) # getting the dictionary of encodings\n",
    "    encode = lambda c: -1 # error handling\n",
    "    try:\n",
    "        # gets the encoding e.g. a = 180, one hots it and\n",
    "        # makes sure it has dimensions np.array.shape(1, len(alphadict))\n",
    "        encode = lambda c: np.array([alphadict[c]])\n",
    "    except KeyError:\n",
    "        print(\"encoding %s was NOT found in dictionary!\" % s)\n",
    "    # concatenating each char in the string to np.array.shape(len(sentence), len(alphadict))\n",
    "    encoding = [encode(c) for c in sentence]\n",
    "    return np.concatenate(encoding, axis=0)\n",
    "\n",
    "def spaces(sentence):\n",
    "    spaces = [np.asarray([idx-1]) for idx, c in enumerate(sentence) if c == \" \"]\n",
    "    spaces.append(np.array([len(sentence)-1]))\n",
    "    spaces = np.concatenate(spaces)\n",
    "    return spaces\n",
    "\n",
    "def char_length(in_string):\n",
    "    return len(in_string)\n",
    "\n",
    "class TextLoadMethod(LoadMethod):\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(\"data/train/europarl-v7.fr-en.en\", \"r\") as f:\n",
    "            self.train_X = f.read().split(\"\\n\")\n",
    "            language = [\"en\" for _ in range(len(self.train_X))]\n",
    "            self.train_X = zip(self.train_X, language)\n",
    "        with open(\"data/train/europarl-v7.fr-en.fr\", \"r\") as f:\n",
    "            self.train_t = f.read().split(\"\\n\")\n",
    "            language = [\"fr\" for _ in range(len(self.train_t))]\n",
    "            self.train_t = zip(self.train_t, language)\n",
    "        self.samples = zip(self.train_X, self.train_t)\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        if self.samples == None:\n",
    "            self._load_data()\n",
    "        self.samples = sorted(self.samples, key=lambda (X, t): len(X)*10000 + len(t))\n",
    "        # remove samples not of interest\n",
    "        self.samples = remove_samples(self.samples)\n",
    "        for sample_idx, sample in enumerate(self.samples):\n",
    "            my_s = []\n",
    "            if (sample_idx % 10000) == 0:\n",
    "                print(\"%d of %d preprocessed ...\"  % (sample_idx, len(self.samples)))\n",
    "            # samples should be tuple((train_X, \"en\") (train_t, \"fr\"))\n",
    "            for elem, lang in sample:\n",
    "                # char encoding\n",
    "                my_s.append(char_encoding(elem, lang))\n",
    "                # spaces\n",
    "                my_s.append(spaces(elem))\n",
    "                # char length\n",
    "                my_s.append(char_length(elem))\n",
    "            self.samples[sample_idx] = tuple(my_s) + sample # concats with original sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1958868 preprocessed ...\n",
      "10000 of 1958868 preprocessed ...\n",
      "20000 of 1958868 preprocessed ...\n",
      "30000 of 1958868 preprocessed ...\n",
      "40000 of 1958868 preprocessed ...\n",
      "50000 of 1958868 preprocessed ...\n",
      "60000 of 1958868 preprocessed ...\n",
      "70000 of 1958868 preprocessed ...\n",
      "80000 of 1958868 preprocessed ...\n",
      "90000 of 1958868 preprocessed ...\n",
      "100000 of 1958868 preprocessed ...\n",
      "110000 of 1958868 preprocessed ...\n",
      "120000 of 1958868 preprocessed ...\n",
      "130000 of 1958868 preprocessed ...\n",
      "140000 of 1958868 preprocessed ...\n",
      "150000 of 1958868 preprocessed ...\n",
      "160000 of 1958868 preprocessed ...\n",
      "170000 of 1958868 preprocessed ...\n",
      "180000 of 1958868 preprocessed ...\n",
      "190000 of 1958868 preprocessed ...\n",
      "200000 of 1958868 preprocessed ...\n",
      "210000 of 1958868 preprocessed ...\n",
      "220000 of 1958868 preprocessed ...\n",
      "230000 of 1958868 preprocessed ...\n",
      "240000 of 1958868 preprocessed ...\n",
      "250000 of 1958868 preprocessed ...\n",
      "260000 of 1958868 preprocessed ...\n",
      "270000 of 1958868 preprocessed ...\n",
      "280000 of 1958868 preprocessed ...\n",
      "290000 of 1958868 preprocessed ...\n",
      "300000 of 1958868 preprocessed ...\n",
      "310000 of 1958868 preprocessed ...\n",
      "320000 of 1958868 preprocessed ...\n",
      "330000 of 1958868 preprocessed ...\n",
      "340000 of 1958868 preprocessed ...\n",
      "350000 of 1958868 preprocessed ...\n",
      "360000 of 1958868 preprocessed ...\n",
      "370000 of 1958868 preprocessed ...\n",
      "380000 of 1958868 preprocessed ...\n",
      "390000 of 1958868 preprocessed ...\n",
      "400000 of 1958868 preprocessed ...\n",
      "410000 of 1958868 preprocessed ...\n",
      "420000 of 1958868 preprocessed ...\n",
      "430000 of 1958868 preprocessed ...\n",
      "440000 of 1958868 preprocessed ...\n",
      "450000 of 1958868 preprocessed ...\n",
      "460000 of 1958868 preprocessed ...\n",
      "470000 of 1958868 preprocessed ...\n",
      "480000 of 1958868 preprocessed ...\n",
      "490000 of 1958868 preprocessed ...\n",
      "500000 of 1958868 preprocessed ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-978fd9993d93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_load_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextLoadMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Frostings-0.1-py2.7.egg/frostings/loader.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2c835e1774b8>\u001b[0m in \u001b[0;36m_preprocess_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mmy_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;31m# spaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mmy_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[1;31m# char length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mmy_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2c835e1774b8>\u001b[0m in \u001b[0;36mspaces\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mspaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mspaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_load_method = TextLoadMethod()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('The third objective is urban and rural development, within the scope of a balanced territorial policy.',\n",
       "  'en'),\n",
       " ('', 'fr'))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_load_method(549)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The third objective is urban and rural development, within the scope of a balanced territorial policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[123]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/alphabet.en', 'r') as f:\n",
    "    john = set(f.read().replace('\\r\\n', '\\n').replace('\\r', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(john)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextLoadMethod(LoadMethod):\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(\"data/train/europarl-v7.fr-en.en\", \"r\") as f:\n",
    "            self.train_X = f.read().split(\"\\n\")\n",
    "        with open(\"data/train/europarl-v7.fr-en.fr\", \"r\") as f:\n",
    "            self.train_t = f.read().split(\"\\n\")\n",
    "        self.samples = zip(self.train_X, self.train_t)\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        if self.samples == None:\n",
    "            self._load_data()\n",
    "        self.samples = sorted(self.samples, key=lambda (X, t): len(X)*10000 + len(t))\n",
    "        # remove samples not of interest\n",
    "        self.samples = remove_samples(self.samples)\n",
    "        for sample_idx, sample in enumerate(self.samples):\n",
    "            my_s = []\n",
    "            for elem in sample:# samples should be tuple(train_X, train_t)\n",
    "                # char encoding\n",
    "                my_s.append(char_encoding(elem))\n",
    "                # spaces\n",
    "                my_s.append(spaces(elem))\n",
    "                # char length\n",
    "                my_s.append(char_length(elem))\n",
    "                # word length\n",
    "                my_s.append(word_length(elem))\n",
    "            self.samples[sample_idx] = tuple(my_s) + sample # concats with original sample\n",
    "\n",
    "def get_max_length(encodings):\n",
    "\tpass\n",
    "\n",
    "class TextBatchGenerator(BatchGenerator):\n",
    "\n",
    "\tdef _make_batch_holder(self, max_length):\n",
    "\t\tself.batch = []\n",
    "\t\tpass # should make a \"holder\", e.g. self.batch.append(np.zeros((self.batch_info.batch_size, max_length, encoding_size) and .append a np.zeros for sequences_lengths, spaces etc.\n",
    "\n",
    "\tdef _make_batch(self):\n",
    "\t\tself._make_batch_holder()\n",
    "\t\tfor _ in range(len(self.samples)):\n",
    "\t\t\tpass # Should fit each sample to the holder\n",
    "\t\treturn self.batch\n",
    "\n",
    "# Chunk loader is not thought of here, but it should fit without modifying it\n",
    "\n",
    "### RUNNING THE TEXT LOADER ###\n",
    "\n",
    "text_load_method = TextLoadMethod()\n",
    "text_load_method(10000) # remember that it has a __call__ function\n",
    "\n",
    "sample_info = SampleInfo(len(text_load_method.samples)) # needs to know how many samples we have, so it can make an idx for all of them.\n",
    "sample_gen = SampleGenerator(text_load_method, sample_info) # generates one sample which consists of several elements sample = (elem, elem, elem)\n",
    "batch_info = BatchInfo(batch_size=32)\n",
    "text_batch_gen = TextBatchGenerator(sample_gen, batch_info) # Generates a batch, being a tuples\n",
    "chunk_info = ChunkInfo()\n",
    "chunk_gen = ChunkGenerator(text_batch_gen, chunk_info)\n",
    "# should be used like.\n",
    "# for train_X_char_enc, train_X_word_enc, train_X_sequence_length ... in text_batch_gen.gen_batch():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
